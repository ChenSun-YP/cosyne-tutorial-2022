{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb23669",
   "metadata": {},
   "source": [
    "# Sound localisation with surrogate gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a4686",
   "metadata": {},
   "source": [
    "## Sound localisation stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd312e52",
   "metadata": {},
   "source": [
    "The following function creates a set of stimuli that can be used for training or testing. It returns two arrays ``ipd`` and ``spikes``. ``ipd`` is an array of length ``num_samples`` that gives the true IPD, and ``spikes`` is an array of 0 (no spike) and 1 (spike) of shape ``(num_samples, duration_steps, 2*anf_per_ear)`` where ``anf_per_ear`` is how many neurons there are receiving the same signal (but generating independent Poisson noise) there are for each ear, and ``duration_steps`` is the number of time steps there are in the stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb26693",
   "metadata": {},
   "outputs": [],
   "source": [
    "second = 1\n",
    "ms = 1e-3\n",
    "Hz = 1\n",
    "\n",
    "# Stimulus and simulation parameters\n",
    "dt = 1*ms          # large time step to make simulations run faster for tutorial\n",
    "anf_per_ear=100    # repeats of each ear with independent noise\n",
    "envelope_power=4   # higher values make sharper envelopes, easier\n",
    "rate_max=1000*Hz   # maximum Poisson firing rate\n",
    "f=20*Hz            # stimulus frequency\n",
    "duration=.1*second # stimulus duration\n",
    "\n",
    "# Generate an input signal (spike array) from array of true IPDs\n",
    "def input_signal(ipd):\n",
    "    num_samples = len(ipd)\n",
    "    duration_steps = int(np.round(duration/dt))\n",
    "    T = np.arange(duration_steps)*dt\n",
    "    phi = 2*np.pi*f*T\n",
    "    theta = np.zeros((num_samples, duration_steps, 2*anf_per_ear))\n",
    "    phase_delays = np.linspace(0, np.pi/2, anf_per_ear)\n",
    "    \n",
    "    theta[:, :, :anf_per_ear] = phi[np.newaxis, :, np.newaxis]+phase_delays[np.newaxis, np.newaxis, :]\n",
    "    theta[:, :, anf_per_ear:] = phi[np.newaxis, :, np.newaxis]+ipd[:, np.newaxis, np.newaxis]+phase_delays[np.newaxis, np.newaxis, ::-1]\n",
    "    spikes = np.random.rand(num_samples, duration_steps, 2*anf_per_ear)<rate_max*dt*(0.5*(1+np.sin(theta)))**envelope_power\n",
    "    return spikes\n",
    "\n",
    "# Generate some true IPDs from U(-pi/2, pi/2) and corresponding spike arrays\n",
    "def random_ipd_input_signal(num_samples):\n",
    "    ipd = np.random.rand(num_samples)*np.pi-np.pi/2 # uniformly random in (-pi/2, pi/2)\n",
    "    spikes = input_signal(ipd)\n",
    "    return ipd, spikes\n",
    "\n",
    "# Plot a few just to show how it looks\n",
    "ipd, spikes = random_ipd_input_signal(8)\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.imshow(spikes[i, :, :].T, aspect='auto', interpolation='nearest', cmap=plt.cm.gray_r)\n",
    "    plt.title(f'True IPD = {int(ipd[i]*180/np.pi)} deg')\n",
    "    if i>=4:\n",
    "        plt.xlabel('Time (steps)')\n",
    "    if i%4==0:\n",
    "        plt.ylabel('Input neuron index')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86605734",
   "metadata": {},
   "source": [
    "Now with this, the aim is to take these input spikes and infer the IPD. We can do this either by discretising and using a classification approach, or with a regression approach. For the moment, let's try it with a classification approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554840b",
   "metadata": {},
   "source": [
    "## Classification approach\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Conversion function\n",
    "* Parameter of number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5456e",
   "metadata": {},
   "source": [
    "## Surrogate gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125002c",
   "metadata": {},
   "source": [
    "First, this is the key part of surrogate gradient descent, a function where we override the computation of the gradient to replace it with a smoothed gradient. You can see that in the forward pass (method ``forward``) it returns the Heaviside function of the input (takes value 1 if the input is ``>0``) or value 0 otherwise. In the backwards pass, it returns the gradient of a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fabc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad = grad_output/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "spike_fn  = SurrGradSpike.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2a95c",
   "metadata": {},
   "source": [
    "## Membrane only (no spiking neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for training. These aren't optimal, but instead designed\n",
    "# to give a reasonable result in a small amount of time for the tutorial!\n",
    "batch_size = 128\n",
    "n_training_batches = 128\n",
    "n_testing_batches = 32\n",
    "num_classes = 180//15 ## classes at 15 degree increments\n",
    "num_samples = batch_size*n_training_batches\n",
    "\n",
    "# Generate the training data\n",
    "# spikes has shape (num_samples, duration_steps, 2*anf_per_ear), ipds has shape (num_samples,)\n",
    "ipds, spikes = random_ipd_input_signal(num_samples)\n",
    "# Convert this\n",
    "ipds = torch.tensor((ipds+np.pi/2)*num_classes/(np.pi), device=device, dtype=torch.long)\n",
    "spikes = torch.tensor(spikes, device=device, dtype=dtype)\n",
    "\n",
    "_, nb_steps, input_size = spikes.shape\n",
    "output_size = num_classes\n",
    "\n",
    "# Filter parameters\n",
    "time_step = 1e-3\n",
    "tau = 20e-3\n",
    "alpha = np.exp(-time_step / tau)\n",
    "\n",
    "# Weights and uniform weight initialisation\n",
    "W = nn.Parameter(torch.empty((input_size, output_size), device=device, dtype=dtype, requires_grad=True))\n",
    "fan_in, _ = nn.init._calculate_fan_in_and_fan_out(W)\n",
    "bound = 1 / math.sqrt(fan_in)\n",
    "nn.init.uniform_(W, -bound, bound)\n",
    "\n",
    "\n",
    "def data_generator(ipds, spikes):\n",
    "    perm = torch.randperm(spikes.shape[0])\n",
    "    spikes = spikes[perm, :, :]\n",
    "    ipds = ipds[perm]\n",
    "    n, _, _ = spikes.shape\n",
    "    n_batch = n//batch_size\n",
    "    for i in range(n_batch):\n",
    "        x_local = spikes[i*batch_size:(i+1)*batch_size, :, :]\n",
    "        y_local = ipds[i*batch_size:(i+1)*batch_size]\n",
    "        yield x_local, y_local\n",
    "\n",
    "        \n",
    "def snn(input_spikes):\n",
    "    # Input has shape (batch_size, nb_steps, input_size)\n",
    "    mem = torch.zeros((batch_size, output_size), device=device, dtype=dtype)\n",
    "\n",
    "    # mem_rec will store the membrane in each time step\n",
    "    mem_rec = [mem]\n",
    "\n",
    "    # Batch matrix multiplication all time steps\n",
    "    # Equivalent to matrix multiply input[b, :, :] x W for all b, but faster\n",
    "    h = torch.einsum(\"abc,cd->abd\", (input_spikes, W))\n",
    "    # Update membrane and spikes one time step at a time\n",
    "    for t in range(nb_steps - 1):\n",
    "        new_mem = (alpha * mem + (1. - alpha) * (h[:, t, :]))\n",
    "        mem = new_mem\n",
    "\n",
    "        mem_rec.append(mem)  # Save the new value\n",
    "\n",
    "    mem_rec = torch.stack(mem_rec, dim=1)  # (batch_size, nb_steps, output_size)\n",
    "    \n",
    "    return mem_rec\n",
    "\n",
    "# Training parameters\n",
    "nb_epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "# Optimiser and loss function\n",
    "optimizer = torch.optim.Adam([W], lr=lr)\n",
    "log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "loss_fn = nn.NLLLoss()\n",
    "# regression: use nn.MSE, \n",
    "\n",
    "print(f\"Want loss for epoch 1 to be about {-np.log(1/num_classes):.2f}, multiply m by constant to get this\")\n",
    "\n",
    "loss_hist = []\n",
    "for e in range(nb_epochs):\n",
    "    local_loss = []\n",
    "    for x_local, y_local in data_generator(ipds, spikes):\n",
    "        # Run network\n",
    "        output = snn(x_local)\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        m = torch.sum(output, 1)*0.01  # Sum time dimension\n",
    "        \n",
    "        loss_ce = loss_fn(log_softmax_fn(m), y_local)\n",
    "\n",
    "        # Compute regularisation loss\n",
    "        loss_reg = 0\n",
    "        # loss_reg = loss_reg_fn([spk_rec1, spk_rec2])\n",
    "        loss = loss_ce + loss_reg\n",
    "        local_loss.append(loss.item())\n",
    "\n",
    "        # Update gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_hist.append(np.mean(local_loss))\n",
    "    print(\"Epoch %i: loss=%.5f\"%(e+1, np.mean(local_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ecfaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc91c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Accuracy\n",
    "accs_train = []\n",
    "for x_local, y_local in data_generator(ipds, spikes):\n",
    "    output = snn(x_local)\n",
    "    m = torch.sum(output, 1)  # Sum time dimension\n",
    "    _, am = torch.max(m, 1)  # argmax over output units\n",
    "    tmp = np.mean((y_local == am).detach().cpu().numpy())  # compare to labels\n",
    "    accs_train.append(tmp)\n",
    "print(f\"Train Accuracy: {100*np.mean(accs_train):.1f}%\")\n",
    "\n",
    "# Test Accuracy\n",
    "ipds_test, spikes_test = random_ipd_input_signal(batch_size*n_testing_batches, **params)\n",
    "ipds_test = torch.tensor((ipds_test+np.pi/2)*num_classes/(np.pi), device=device, dtype=torch.long)\n",
    "spikes_test = torch.tensor(spikes_test, device=device, dtype=dtype)\n",
    "accs_test = []\n",
    "ipd_true = []\n",
    "ipd_est = []\n",
    "confusion = np.zeros((num_classes, num_classes))\n",
    "for x_local, y_local in data_generator(ipds_test, spikes_test):\n",
    "    output = snn(x_local)\n",
    "    m = torch.sum(output, 1)  # Sum time dimension\n",
    "    _, am = torch.max(m, 1)  # argmax over output units\n",
    "    tmp = np.mean((y_local == am).detach().cpu().numpy())  # compare to labels\n",
    "    for i, j in zip(y_local.detach().cpu().numpy(), am.detach().cpu().numpy()):\n",
    "        confusion[j, i] += 1\n",
    "    ipd_true.append(y_local.detach().cpu().numpy()/num_classes*np.pi-np.pi/2)\n",
    "    ipd_est.append(am.detach().cpu().numpy()/num_classes*np.pi-np.pi/2)\n",
    "    accs_test.append(tmp)\n",
    "ipd_true = np.hstack(ipd_true)\n",
    "ipd_est = np.hstack(ipd_est)\n",
    "abs_errors_deg = abs(ipd_true-ipd_est)*180/np.pi\n",
    "print(f\"Test Accuracy: {100*np.mean(accs_test):.1f}%\")\n",
    "print(f\"Chance level: {100*1/num_classes:.1f}%\")\n",
    "print(f\"Absolute error: {np.mean(abs_errors_deg):.1f} deg\")\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.subplot(121)\n",
    "plt.hist(ipd_true*180/np.pi, bins=num_classes, label='True')\n",
    "plt.hist(ipd_est*180/np.pi, bins=num_classes, label='Estimated')\n",
    "plt.xlabel(\"IPD\")\n",
    "plt.yticks([])\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(122)\n",
    "confusion /= np.sum(confusion, axis=0)[np.newaxis, :]\n",
    "plt.imshow(confusion, interpolation='nearest', aspect='auto', origin='lower', extent=(-90, 90, -90, 90))\n",
    "plt.xlabel('True IPD')\n",
    "plt.ylabel('Estimated IPD')\n",
    "plt.title('Confusion matrix')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a0176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(W.detach().cpu().numpy(), interpolation='nearest', aspect='auto', origin='lower')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
